{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "h.nn.rnn-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxukuQSSelKAFcMmyMrYnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sychun/colab/blob/master/h_nn_rnn_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yZRiiompdMl"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip\n",
        "# !head glove.6B.100d.txt\n",
        "# ! rm -f glove.6B.zip\n",
        "# ! rm -f glove.6B.300d.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2faKYzqhbP",
        "outputId": "b333e60c-48a5-488d-9545-bc7bc4d7ea29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(('glove.6B.100d.txt'), encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0] # 단어\n",
        "    coefs = np.asarray(values[1:], dtype='float32') # 가중치\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('%s개의 단어 벡터를 찾았습니다.' % len(embeddings_index))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000개의 단어 벡터를 찾았습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2LCzmHtS0E",
        "outputId": "e8d483da-28de-44da-a544-e01d50fc56c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "print( embeddings_index['the'].shape)\n",
        "print( embeddings_index['the'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100,)\n",
            "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
            "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
            " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
            " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
            " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
            "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
            "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
            " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
            "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
            "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
            "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
            " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
            " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
            " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
            "  0.8278    0.27062 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5ifOjFFcqyy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMJQM5N5_gOY",
        "outputId": "f25f79e8-f0e0-45df-b3b3-9b25e54ec17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# !wget http://mng.bz/0tIo --no-check-certificate\n",
        "# !mv 0tIo aclImdb.zip # 이름 변경\n",
        "# !unzip aclImdb.zip\n",
        "# !rm -f aclImdb.zip\n",
        "# !ls -l"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1183520\n",
            "drwxr-xr-x 4 root root      4096 Nov 18  2017 aclImdb\n",
            "-rw-rw-r-- 1 root root 347116733 Aug  4  2014 glove.6B.100d.txt\n",
            "-rw-rw-r-- 1 root root 693432828 Aug  4  2014 glove.6B.200d.txt\n",
            "-rw-rw-r-- 1 root root 171350079 Aug  4  2014 glove.6B.50d.txt\n",
            "drwxrwxr-x 3 root root      4096 Nov 18  2017 __MACOSX\n",
            "drwxr-xr-x 1 root root      4096 Sep 16 16:29 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij-i9QmxdWn2"
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = './aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels2 = []\n",
        "texts2 = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
        "            texts2.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels2.append(0)\n",
        "            else:\n",
        "                labels2.append(1)\n",
        "\n",
        "\n",
        "\n",
        "# sequences2 = tokenizer.texts_to_sequences(texts2)\n",
        "# x_test = pad_sequences(sequences2, maxlen=maxlen)\n",
        "# y_test = np.asarray(labels2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBeA7bGMd1G4",
        "outputId": "cfd2e941-2589-48d3-c387-46f38f7ba066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(len(labels), labels[:5], labels[-5:])\n",
        "print(len(texts))\n",
        "texts[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 [0, 0, 0, 0, 0] [1, 1, 1, 1, 1]\n",
            "25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Why else would he do this to me?<br /><br />Not that I expect Dean Cain to produce hit movies. Or even decent movies. I saw Lois and Clark, I am aware of just how... \"good\" Dean Cain is.<br /><br />Obviously this is gonna be a cheesey flick, and each cheesey flick has its own special way to make you scratch your head. I will not call these spoilers as you can\\'t really spoil this movie any more than it already is.<br /><br />To begin with... why is that a fake helicopter? I mean... why?<br /><br />How come that one scientist is from Chicago and that other scientist is from LA and neither one could be any more eastern european if they tried? How hard would it have been to get either an american actor, or just change that lame state sheet the movie provides us with to say those people aren\\'t american?<br /><br />Why are there 2 occasions when the movie gives us a slug line? We get helipad-day and then mess hall-day later on. And then that\\'s it, who cares about the timeline. To be honest, who cared about it even when they mentioned it, but I guess that\\'s beside the point.<br /><br />Does a movie really get better if you are able to view it through multiple split screens? The answer is no.<br /><br />That dragon sure can walk down that hall..over..and over...and over....and over...<br /><br />Who on earth was responsible for one of the worst endings in film history? It was straight out of scooby doo. Oh, the dragon\\'s dead now...say, wanna get dinner? Sure, but not at some Chinese place....with Dragon in the name!! AH HA HA HA!! HA HA HA!! HAHA HA! I used to be Superman! AHA HA HA! HA HA!<br /><br />fade to black<br /><br />my god, it made me cringe it was so stupid.<br /><br />But never fear..even though the whole building exploded...and no one was left alive..for some reason there\\'s a second untouched, unmanned lab that survived pretty well, so they can make a sequel. Hurray for us all.<br /><br />'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ovrTuGeeaD",
        "outputId": "2c90e2dc-d3f7-49f7-c7dc-d5bf20ab4c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # 100개 단어 이후는 버립니다\n",
        "training_samples = 200  # 훈련 샘플은 200개입니다\n",
        "validation_samples = 10000  # 검증 샘플은 10,000개입니다\n",
        "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용합니다\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts) # 문자열 -> 숫자 변경 학습\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen) # ? x 100 행렬\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('데이터 텐서의 크기:', data.shape)\n",
        "print('레이블 텐서의 크기:', labels.shape)\n",
        "\n",
        "# 데이터를 훈련 세트와 검증 세트로 분할합니다.\n",
        "# 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후에 긍정 샘플이 옵니다) \n",
        "# 먼저 데이터를 섞습니다.\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "\n",
        "sequences2 = tokenizer.texts_to_sequences(texts2)\n",
        "x_test = pad_sequences(sequences2, maxlen=maxlen)\n",
        "y_test = np.asarray(labels2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88582개의 고유한 토큰을 찾았습니다.\n",
            "데이터 텐서의 크기: (25000, 100)\n",
            "레이블 텐서의 크기: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyi52vy6eewI",
        "outputId": "2c6c486f-87f4-4e39-a06c-465489c7429d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# texts[0]\n",
        "# sequences[0]\n",
        "# data.shape\n",
        "# data[0]\n",
        "word_index['moscow']"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-_XaUBeh2HH"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # 임베딩 인덱스에 없는 단어는 모두 0이 됩니다.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGzlliRZh4PB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBl8S6BFh1zj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK9Wva_qtoqe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13EVVnmt_voe",
        "outputId": "5896b35a-e407-46e9-d7c8-17a2aff617ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "#                               전체단어수, 학습될 단어별 속성 수, 입력 데이터에 포함될 단어 수. 출력: 100 x 100\n",
        "model.add(keras.layers.Embedding(10000, 100, input_length=100))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3CdpeAA_aP"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix]) # 이미 잘 학습된 가중치를 모델에 저장\n",
        "model.layers[0].trainable = False # 이미 잘 학습되어 있으니, 첫 레이어는 train하지 말 것"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DHgZHnuigf0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}